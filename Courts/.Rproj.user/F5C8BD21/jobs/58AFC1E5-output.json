[1,"── \u001B[1mAttaching core tidyverse packages\u001B[22m ──────────────────────── tidyverse 2.0.0 ──\n"]
[1,"\u001B[32m✔\u001B[39m \u001B[34mdplyr    \u001B[39m 1.1.3     \u001B[32m✔\u001B[39m \u001B[34mreadr    \u001B[39m 2.1.4\n"]
[1,"\u001B[32m✔\u001B[39m \u001B[34mforcats  \u001B[39m 1.0.0     \u001B[32m✔\u001B[39m \u001B[34mstringr  \u001B[39m 1.5.0\n"]
[1,"\u001B[32m✔\u001B[39m \u001B[34mggplot2  \u001B[39m 3.4.4     \u001B[32m✔\u001B[39m \u001B[34mtibble   \u001B[39m 3.2.1\n"]
[1,"\u001B[32m✔\u001B[39m \u001B[34mlubridate\u001B[39m 1.9.3     \u001B[32m✔\u001B[39m \u001B[34mtidyr    \u001B[39m 1.3.0\n"]
[1,"\u001B[32m✔\u001B[39m \u001B[34mpurrr    \u001B[39m 1.0.2     \n"]
[1,"── \u001B[1mConflicts\u001B[22m ────────────────────────────────────────── tidyverse_conflicts() ──\n"]
[1,"\u001B[31m✖\u001B[39m \u001B[34mdplyr\u001B[39m::\u001B[32mfilter()\u001B[39m masks \u001B[34mstats\u001B[39m::filter()\n"]
[1,"\u001B[31m✖\u001B[39m \u001B[34mdplyr\u001B[39m::\u001B[32mlag()\u001B[39m    masks \u001B[34mstats\u001B[39m::lag()\n"]
[1,"\u001B[36mℹ\u001B[39m Use the \u001B]8;;http://conflicted.r-lib.org/\u0007conflicted package\u001B]8;;\u0007 to force all conflicts to become errors\n"]
[2,"\nAttaching package: ‘jsonlite’\n\nThe following object is masked from ‘package:purrr’:\n\n    flatten\n\n"]
[2,"\nAttaching package: ‘sparklyr’\n\nThe following object is masked from ‘package:purrr’:\n\n    invoke\n\nThe following object is masked from ‘package:stats’:\n\n    filter\n\n"]
[2,"\nAttaching package: ‘arrow’\n\nThe following object is masked from ‘package:lubridate’:\n\n    duration\n\nThe following object is masked from ‘package:utils’:\n\n    timestamp\n\n"]
[1,"\u001B[1m\u001B[33mError\u001B[39m:\u001B[22m\n"]
[1,"\u001B[1m\u001B[22m\u001B[33m!\u001B[39m org.apache.spark.SparkException: Job aborted due to stage failure:\n"]
[1,"  Task 3 in stage 567.0 failed 1 times, most recent failure: Lost task 3.0 in\n"]
[1,"  stage 567.0 (TID 3336) (localhost executor driver):\n"]
[1,"  java.lang.OutOfMemoryError: Java heap space\n"]
[1,"Run \u001B]8;;x-r-run:sparklyr::spark_last_error()\u0007`sparklyr::spark_last_error()`\u001B]8;;\u0007 to see the full Spark error (multiple lines)\n"]
[1,"To use the previous style of error message set\n"]
[1,"`options(\"sparklyr.simple.errors\" = TRUE)`\n"]
[1,"Backtrace:\n"]
[1,"\u001B[90m     \u001B[39m▆\n"]
[1,"\u001B[90m  1. \u001B[39m├─.rs.sourceWithProgress(...)\n"]
[1,"\u001B[90m  2. \u001B[39m│ └─\u001B[1mbase\u001B[22m::eval(statements[[idx]], envir = globalenv())\n"]
[1,"\u001B[90m  3. \u001B[39m│   └─base::eval(statements[[idx]], envir = globalenv())\n"]
[1,"\u001B[90m  4. \u001B[39m├─\u001B[1mdplyr\u001B[22m::collect(slice_sample(select(na.omit(final), !text), n = 10000))\n"]
[1,"\u001B[90m  5. \u001B[39m├─dplyr::slice_sample(select(na.omit(final), !text), n = 10000)\n"]
[1,"\u001B[90m  6. \u001B[39m├─dplyr::select(na.omit(final), !text)\n"]
[1,"\u001B[90m  7. \u001B[39m├─\u001B[1mstats\u001B[22m::na.omit(final)\n"]
[1,"\u001B[90m  8. \u001B[39m└─\u001B[1msparklyr\u001B[22m:::na.omit.tbl_spark(final)\n"]
[1,"\u001B[90m  9. \u001B[39m  ├─\u001B[1mstats\u001B[22m::na.omit(spark_dataframe(object), columns = NULL, ...)\n"]
[1,"\u001B[90m 10. \u001B[39m  └─\u001B[1msparklyr\u001B[22m:::na.omit.spark_jobj(...)\n"]
[1,"\u001B[90m 11. \u001B[39m    ├─sparklyr::invoke(dropped, \"count\")\n"]
[1,"\u001B[90m 12. \u001B[39m    └─sparklyr:::invoke.shell_jobj(dropped, \"count\")\n"]
[1,"\u001B[90m 13. \u001B[39m      ├─sparklyr::invoke_method(...)\n"]
[1,"\u001B[90m 14. \u001B[39m      └─sparklyr:::invoke_method.spark_shell_connection(...)\n"]
[1,"\u001B[90m 15. \u001B[39m        └─sparklyr:::core_invoke_method(...)\n"]
[1,"\u001B[90m 16. \u001B[39m          └─sparklyr:::core_invoke_method_impl(...)\n"]
[1,"\u001B[90m 17. \u001B[39m            └─sparklyr:::spark_error(msg)\n"]
[1,"\u001B[90m 18. \u001B[39m              └─\u001B[1mrlang\u001B[22m::abort(message = msg, use_cli_format = TRUE, call = NULL)\n"]
[2,"Warning message:\nIn sprintf(versions$pattern, version$spark, version$hadoop) :\n  2 arguments not used by format 'spark-3.4.1-bin-hadoop3'\n"]
[1,"\u001B[1m\u001B[33mError\u001B[39m:\u001B[22m\n"]
[1,"\u001B[1m\u001B[22m\u001B[33m!\u001B[39m java.lang.IllegalStateException: Cannot call methods on a stopped\n"]
[1,"  SparkContext. This stopped SparkContext was created at:\n"]
[1,"org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:1020)\n"]
[1,"  sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n"]
[1,"  sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n"]
[1,"  sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n"]
[1,"  java.lang.reflect.Method.invoke(Method.java:498)\n"]
[1,"  sparklyr.Invoke.invoke(invoke.scala:161)\n"]
[1,"  sparklyr.StreamHandler.handleMethodCall(stream.scala:141)\n"]
[1,"  sparklyr.StreamHandler.read(stream.scala:62)\n"]
[1,"  sparklyr.BackendHandler.$anonfun$channelRead0$1(handler.scala:60)\n"]
[1,"  scala.util.control.Breaks.breakable(Breaks.scala:42)\n"]
[1,"  sparklyr.BackendHandler.channelRead0(handler.scala:41)\n"]
[1,"  sparklyr.BackendHandler.channelRead0(handler.scala:14)\n"]
[1,"  io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:99)\n"]
[1,"  io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)\n"]
[1,"  io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)\n"]
[1,"  io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)\n"]
[1,"  io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103)\n"]
[1,"  io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)\n"]
[1,"  io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)\n"]
[1,"  io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)\n"]
[1,"The currently active SparkContext was created at:\n"]
[1,"org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:1020)\n"]
[1,"  sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n"]
[1,"  sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n"]
[1,"  sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n"]
[1,"  java.lang.reflect.Method.invoke(Method.java:498)\n"]
[1,"  sparklyr.Invoke.invoke(invoke.scala:161)\n"]
[1,"  sparklyr.StreamHandler.handleMethodCall(stream.scala:141)\n"]
[1,"  sparklyr.StreamHandler.read(stream.scala:62)\n"]
[1,"  sparklyr.BackendHandler.$anonfun$channelRead0$1(handler.scala:60)\n"]
[1,"  scala.util.control.Breaks.breakable(Breaks.scala:42)\n"]
[1,"  sparklyr.BackendHandler.channelRead0(handler.scala:41)\n"]
[1,"  sparklyr.BackendHandler.channelRead0(handler.scala:14)\n"]
[1,"  io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:99)\n"]
[1,"  io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)\n"]
[1,"  io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)\n"]
[1,"  io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)\n"]
[1,"  io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103)\n"]
[1,"  io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)\n"]
[1,"  io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)\n"]
[1,"  io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)\n"]
[1,"Run \u001B]8;;x-r-run:sparklyr::spark_last_error()\u0007`sparklyr::spark_last_error()`\u001B]8;;\u0007 to see the full Spark error (multiple lines)\n"]
[1,"To use the previous style of error message set\n"]
[1,"`options(\"sparklyr.simple.errors\" = TRUE)`\n"]
[1,"Backtrace:\n"]
[1,"\u001B[90m     \u001B[39m▆\n"]
[1,"\u001B[90m  1. \u001B[39m├─.rs.sourceWithProgress(...)\n"]
[1,"\u001B[90m  2. \u001B[39m│ └─\u001B[1mbase\u001B[22m::eval(statements[[idx]], envir = globalenv())\n"]
[1,"\u001B[90m  3. \u001B[39m│   └─base::eval(statements[[idx]], envir = globalenv())\n"]
[1,"\u001B[90m  4. \u001B[39m├─\u001B[1mdplyr\u001B[22m::collect(slice_sample(select(na.omit(final), !text), n = 10000))\n"]
[1,"\u001B[90m  5. \u001B[39m├─dplyr::slice_sample(select(na.omit(final), !text), n = 10000)\n"]
[1,"\u001B[90m  6. \u001B[39m├─dplyr::select(na.omit(final), !text)\n"]
[1,"\u001B[90m  7. \u001B[39m├─\u001B[1mstats\u001B[22m::na.omit(final)\n"]
[1,"\u001B[90m  8. \u001B[39m└─\u001B[1msparklyr\u001B[22m:::na.omit.tbl_spark(final)\n"]
[1,"\u001B[90m  9. \u001B[39m  ├─\u001B[1mstats\u001B[22m::na.omit(spark_dataframe(object), columns = NULL, ...)\n"]
[1,"\u001B[90m 10. \u001B[39m  └─\u001B[1msparklyr\u001B[22m:::na.omit.spark_jobj(...)\n"]
[1,"\u001B[90m 11. \u001B[39m    ├─sparklyr::invoke(dropped, \"count\")\n"]
[1,"\u001B[90m 12. \u001B[39m    └─sparklyr:::invoke.shell_jobj(dropped, \"count\")\n"]
[1,"\u001B[90m 13. \u001B[39m      ├─sparklyr::invoke_method(...)\n"]
[1,"\u001B[90m 14. \u001B[39m      └─sparklyr:::invoke_method.spark_shell_connection(...)\n"]
[1,"\u001B[90m 15. \u001B[39m        └─sparklyr:::core_invoke_method(...)\n"]
[1,"\u001B[90m 16. \u001B[39m          └─sparklyr:::core_invoke_method_impl(...)\n"]
[1,"\u001B[90m 17. \u001B[39m            └─sparklyr:::core_invoke_cancel_running(sc)\n"]
[1,"\u001B[90m 18. \u001B[39m              └─sparklyr:::connection_progress_context(...)\n"]
[1,"\u001B[90m 19. \u001B[39m                └─sparklyr (local) f()\n"]
[1,"\u001B[90m 20. \u001B[39m                  ├─sparklyr::invoke(spark_context(sc), \"cancelAllJobs\")\n"]
[1,"\u001B[90m 21. \u001B[39m                  └─sparklyr:::invoke.shell_jobj(spark_context(sc), \"cancelAllJobs\")\n"]
[1,"\u001B[90m 22. \u001B[39m                    ├─sparklyr::invoke_method(...)\n"]
[1,"\u001B[90m 23. \u001B[39m                    └─sparklyr:::invoke_method.spark_shell_connection(...)\n"]
[1,"\u001B[90m 24. \u001B[39m                      └─sparklyr:::core_invoke_method(...)\n"]
[1,"\u001B[90m 25. \u001B[39m                        └─sparklyr:::core_invoke_method_impl(...)\n"]
[1,"\u001B[90m 26. \u001B[39m                          └─sparklyr:::spark_error(msg)\n"]
[1,"\u001B[90m 27. \u001B[39m                            └─\u001B[1mrlang\u001B[22m::abort(message = msg, use_cli_format = TRUE, call = NULL)\n"]
[2,"Execution halted\n"]
